<div align="center">
  <p align="center">
    ùïè <a href="https://twitter.com/maximelabonne">Follow me on X</a> ‚Ä¢ 
    ü§ó <a href="https://huggingface.co/mlabonne">Hugging Face</a> ‚Ä¢ 
    üíª <a href="https://mlabonne.github.io/blog">Blog</a> ‚Ä¢ 
    üìô <a href="https://github.com/PacktPublishing/LLM-Engineers-Handbook">LLM Engineer's Handbook</a>
  </p>
</div>
<br/>

Hi, I'm a Machine Learning Scientist, Author, Blogger, and LLM Developer.

## üíº Projects

* [**The LLM Course**](https://github.com/mlabonne/llm-course): A popular curated list of resources to get into LLMs (>65k ‚≠ê).
* [**LLM Datasets**](https://github.com/mlabonne/llm-datasets): Curated list of high-quality datasets for LLM fine-tuning.
* [**LLM Tools**](https://github.com/mlabonne/llm-course?tab=readme-ov-file#tools): Automate LLM pipelines with Colab notebooks like [LLM AutoEval](https://github.com/mlabonne/llm-autoeval), [LazyMergekit](https://colab.research.google.com/drive/1obulZ1ROXHjYLn6PPZJwRR6GzgQogxxb?usp=sharing), [LazyAxolotl](https://colab.research.google.com/drive/1TsDKNo2riwVmU55gjuBgB1AXVtRRfRHW?usp=sharing), and [AutoQuant](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing).

## ü§ó Models

* [**LFM2**](https://www.liquid.ai/liquid-foundation-models): My work at Liquid AI is to post-train our own pre-trained LLMs with a custom architecture. [[Playground]](https://playground.liquid.ai/chat)
* [**Abliterated models**](https://huggingface.co/collections/mlabonne/abliteration-66bf9a0f9f88f7346cb9462f): Collection of abliterated models to remove refusals. [[Article]](https://huggingface.co/blog/mlabonne/abliteration)
* [**Fine-tunes & Merges**](https://huggingface.co/mlabonne/): Popular models like [NeuralDaredevil-8B](https://huggingface.co/mlabonne/NeuralDaredevil-8B-abliterated), [AlphaMonarch-7B](https://huggingface.co/mlabonne/AlphaMonarch-7B), [NeuralBeagle14-7B](https://huggingface.co/mlabonne/NeuralBeagle14-7B), or [NeuralHermes](https://huggingface.co/mlabonne/NeuralHermes-2.5-Mistral-7B) (first successful open-source DPO) and MoEs like [Phixtral](https://huggingface.co/mlabonne/phixtral-2x2_8) (first Phi-based MoE) and [Beyonder-4x7B-v3](https://huggingface.co/mlabonne/Beyonder-4x7B-v3).

## üìö Books

* [**LLM Engineer's Handbook**](https://github.com/PacktPublishing/LLM-Engineers-Handbook): Practical guide about LLM engineering with fine-tuning, RAG, data, evaluation, deployment, and more!
* [**Hands-on GNNs**](https://github.com/PacktPublishing/Hands-On-Graph-Neural-Networks-Using-Python): Technical book on how to design and implement many types of graph neural networks for various use cases.
